apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pfnano-process-
spec:
  entrypoint: cms-od-example
  serviceAccountName: argo-service-account
  
  arguments:
    parameters:
    - name: nEvents   
      #FIXME 
      # Number of events in the dataset to be processed (-1 is all)                     
      value: 100
    - name: recid
      #FIXME
      # Record id of the dataset to be processed
      value: 30511
    - name: nJobs
      #FIXME 
      # Number of jobs the processing workflow should be split into
      value: 1
    - name: bucket
      #FIXME 
      # Name of cloud storage bucket for storing outputs
      value: mystorage

  templates:
  - name: cms-od-example
    inputs:
      parameters:
      - name: nEvents
      - name: recid
      - name: nJobs
      - name: bucket
    dag:
      tasks:

      - name: get-metadata
        template: get-metadata-template
        arguments:
         parameters:
          - name: recid
            value: "{{inputs.parameters.recid}}"
          - name: bucket
            value: "{{inputs.parameters.bucket}}"
          - name: dataType
            value: "{{outputs.parameters.dataType}}"

      - name: joblist
        dependencies: [get-metadata]
        template: joblist-template
        arguments:
         parameters:
          - name: nJobs
            value: "{{inputs.parameters.nJobs}}"
          - name: nEvents
            value: "{{inputs.parameters.nEvents}}"
          - name: totFiles
            value: "{{tasks.get-metadata.outputs.result}}"

      - name: runpfnano
        dependencies: [joblist]
        template: runpfnano-template
        arguments:
         parameters:
          - name: recid
            value: "{{inputs.parameters.recid}}"
          - name: bucket
            value: "{{inputs.parameters.bucket}}"
          - name: dataType
            value: "{{tasks.get-metadata.outputs.parameters.dataType}}"
          - name: it
            value: "{{item.it}}"
          - name: firstFile
            value: "{{item.firstfile}}"
          - name: lastFile
            value: "{{item.lastfile}}" 
          - name: eventsInJob
            value: "{{item.eventsinjob}}"
        withParam: "{{tasks.joblist.outputs.result}}"
              
  # Get the metadata of the dataset
  # Accidentally showing three different ways of passing parameters/files btw the steps
  # - the full list of files: write it to a file in the GCS bucket
  # - the type of data: write it to the step's ouput parameter "{{tasks.get-metadata.outputs.parameters.dataType}}#   (through a temporary file /tmp/type.txt)
  # - the total number of files which is the stdout output of this step and goes to {{tasks.get-metadata.outputs.result}}
  - name: get-metadata-template
    inputs:
      parameters:
      - name: recid
      - name: bucket
    outputs:
      parameters:
      - name: dataType
        valueFrom:
          default: "default"
          path: /tmp/type.txt
      artifacts:
      - name: filelist
        path: bucket
        s3:
          endpoint: s3.pub1.infomaniak.cloud # from https://docs.infomaniak.cloud/object_storage/s3/
          bucket: "{{inputs.parameters.bucket}}"
          region: us-east-1  # placeholder, Infomaniak storage is in Switzerland
          accessKeySecret:   # created with: openstack ec2 credentials create (take access, secret)
            name: s3-credentials
            key: S3_ACCESS_KEY_ID
          secretKeySecret:
            name: s3-credentials
            key: S3_SECRET_ACCESS_KEY
          insecure: false
          key: pfnano/{{inputs.parameters.recid}}/files_{{inputs.parameters.recid}}.txt
    script:
      image: cernopendata/cernopendata-client
      command: [bash]
      source: |
        mkdir bucket
        cernopendata-client get-file-locations --recid "{{inputs.parameters.recid}}" --protocol xrootd > bucket/files_{{inputs.parameters.recid}}.txt;
        cernopendata-client get-metadata --recid "{{inputs.parameters.recid}}"  --output-value type.secondary > /tmp/type.txt 
        cernopendata-client get-metadata --recid "{{inputs.parameters.recid}}"  --output-value distribution.number_files
        
  # Generate the iterator list for the scatter step
  # Compute the number of events and files for each step
  # Write out the list with first and last filenumbers and the numebr of events to be taken as the input of the following steps
  # (see {{tasks.joblist.outputs.result}} as "withParam" in runpfnano-template)
  - name: joblist-template
    inputs:
      parameters:
      - name: nJobs
      - name: nEvents
      - name: totFiles
    script:
      image: python:alpine3.6
      command: [python]
      source: |
        import json
        import sys
        def split(a, n):
          k, m = divmod(len(a), n)
          return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))
        nJobs = {{inputs.parameters.nJobs}}
        nEvents = {{inputs.parameters.nEvents}}
        totFiles = {{inputs.parameters.totFiles}}
        filesInJob = int(totFiles/nJobs)
        modEvents = 0
        eventsInJob = -1
        if nEvents > 0:
          eventsInJob = int(nEvents/nJobs)
          modEvents = nEvents % nJobs
        splits= list(split(range(totFiles),nJobs))
        itlist = [i for i in range(1, nJobs+1)]
        dictlist = []
        for i in itlist:
          first = 1+splits[i-1][0]
          last = 1+splits[i-1][len(splits[i-1])-1]
          adict = { "it": i, 
                    "firstfile": first, 
                    "lastfile":  last,
                    "eventsinjob": eventsInJob}
          if i == nJobs:            
            adict = { "it": i, 
                      "firstfile": first, 
                      "lastfile":  last,
                      "eventsinjob": eventsInJob + modEvents}
          dictlist.append(adict)
        json.dump(dictlist, sys.stdout)

  # Run the CMSSW step
  # This iterates over the list that it gets as "withParam"
  - name: runpfnano-template
    inputs:
      parameters:
      - name: it
      - name: firstFile
      - name: lastFile
      - name: recid
      - name: bucket
      - name: dataType
      - name: eventsInJob
      artifacts:
      - name: bucket-input
        path: /code/filelist
        s3:
          endpoint: s3.pub1.infomaniak.cloud # from https://docs.infomaniak.cloud/object_storage/s3/
          bucket: "{{inputs.parameters.bucket}}"
          region: us-east-1  # placeholder, Infomaniak storage is in Switzerland
          accessKeySecret:   # created with: openstack ec2 credentials create (take access, secret)
            name: s3-credentials
            key: S3_ACCESS_KEY_ID
          secretKeySecret:
            name: s3-credentials
            key: S3_SECRET_ACCESS_KEY
          insecure: false
          key: pfnano/{{inputs.parameters.recid}}/files_{{inputs.parameters.recid}}.txt
    outputs:
      artifacts:
      - name: filelist
        path: /code/scatter
        archive:
          none: {}
        s3:
          endpoint: s3.pub1.infomaniak.cloud # from https://docs.infomaniak.cloud/object_storage/s3/
          bucket: "{{inputs.parameters.bucket}}"
          region: us-east-1  # placeholder, Infomaniak storage is in Switzerland
          accessKeySecret:   # created with: openstack ec2 credentials create (take access, secret)
            name: s3-credentials
            key: S3_ACCESS_KEY_ID
          secretKeySecret:
            name: s3-credentials
            key: S3_SECRET_ACCESS_KEY
          insecure: false  
          key: pfnano/{{inputs.parameters.recid}}/scatter/
      - name: logs
        path: /code/logs
        archive:
          none: {}
        s3:
          endpoint: s3.pub1.infomaniak.cloud # from https://docs.infomaniak.cloud/object_storage/s3/
          bucket: "{{inputs.parameters.bucket}}"
          region: us-east-1  # placeholder, Infomaniak storage is in Switzerland
          accessKeySecret:   # created with: openstack ec2 credentials create (take access, secret)
            name: s3-credentials
            key: S3_ACCESS_KEY_ID
          secretKeySecret:
            name: s3-credentials
            key: S3_SECRET_ACCESS_KEY
          insecure: false
          key: pfnano/{{inputs.parameters.recid}}/logs/
    script: 
      image: ghcr.io/cms-dpoa/pfnano-image-build:main
      command: [bash]
      source: | 
        
        # sudo chown $USER /mnt/vol
        mkdir /code/scatter
        mkdir /code/logs
        source /opt/cms/entrypoint.sh
        eval `scramv1 runtime -sh`
        # git clone https://github.com/cms-opendata-analyses/PFNanoProducerTool.git PhysicsTools/PFNano
        cd /code/CMSSW_10_6_30/src/PhysicsTools/PFNano
        # scram b

        it="{{inputs.parameters.it}}"
        eventsString="{{inputs.parameters.eventsInJob}}"
        eventsInJob="int($eventsString)"
        dataType="{{inputs.parameters.dataType}}"
        echo Datatype $dataType
        isData=False
        if  echo $dataType | grep Collision ; then isData=True; fi

        firstFile="{{inputs.parameters.firstFile}}"
        lastFile="{{inputs.parameters.lastFile}}"
        echo firstFile $firstFile
        echo lastFile $lastFile

        sed -i '/process.options.num/s/^/#/g' pfnano_data_2016UL_OpenData_cloud.py
        sed -i "/process.load('FWCore.MessageService.MessageLogger_cfi')/a process.MessageLogger.cerr.FwkReport.reportEvery = 500" pfnano_data_2016UL_OpenData_cloud.py
        sed -i "/from Configuration.AlCa.GlobalTag import GlobalTag/a process.GlobalTag.connect = cms.string('sqlite_file:/cvmfs/cms-opendata-conddb.cern.ch/106X_dataRun2_v37.db')" pfnano_data_2016UL_OpenData_cloud.py
        #sed -i "/Customisation from command line/a process.add_(cms.Service("SiteLocalConfigService", overrideSourceTimeout = cms.untracked.uint32(6*60)))" pfnano_data_2016UL_OpenData_cloud.py

        cms_start_time=$(date +%s)
        cmsRun pfnano_data_2016UL_OpenData_cloud.py $firstFile $lastFile '"/code/filelist/files_{{inputs.parameters.recid}}.txt"' $eventsInJob  2>&1 | tee $it.logs
        cms_end_time=$(date +%s)
        job_duration=$((cms_end_time - cms_start_time))
        echo "Job duration: Job $it took $(date -u -d @${job_duration} +'%H:%M:%S') to complete."
        echo -e "Output file info: $(ls -lh nano_data2016.root)"
        
        cp $it.logs /code/logs
        mv nano_data2016.root /code/scatter/pfnanooutput$it.root
      resources:
        requests:
          cpu: "480m"
          memory: "1.1Gi"
          ephemeral-storage: "2Gi"


